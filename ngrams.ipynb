{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to gather Ngrams from google books\n",
    "\n",
    "### Data source\n",
    "The data is take from [googles ngrams](http://storage.googleapis.com/books/ngrams/books/datasetsv3.html)\n",
    "\n",
    "The ngrams are stored as large compressed files that are sorted and split by size.\n",
    "\n",
    "The format is: ngram TAB year TAB match_count TAB volume_count (Repeat) TAB year TAB match_count TAB volume_count, ... \n",
    "\n",
    "Each files size is around 400MB - 500MB and around 1400MB if you extract it for the 2grams. As n changes, the file size and amount may change. n = 1 has 24, n = 2 has 489,  n = 3 has 6881, n = 4 has 6668, n = 5 has 19423 files.\n",
    "\n",
    "### Extraction method \n",
    "\n",
    "I used python to get the data. I processed the data one file at a time with a buffer without saving the file. I processed each file selecting words without parts of speech tags and summing the total occurrence from 1990 to February 2020. I then used a priority queue and took the top 1000 items. I then saved the file. There is an example of an intermediate random file for the bigram showing the occurrence then the ngram:\n",
    "\n",
    "A Pr intermediate file for bigram:\n",
    "    (14561073, 'Proceedings of'),\n",
    "    (10833244, 'Prime Minister'),\n",
    "    (10588830, 'Proc .'),\n",
    "    (7036010, 'Prior to'),\n",
    "    (6596211, 'Professor of'),\n",
    "    (5769636, 'Program ,'),\n",
    "    (5600473, 'Princeton University'),\n",
    "    (4533204, 'Program .'),\n",
    "\n",
    "I will mention that I skipped over _ symbol because there are part of speech tags within the data that I am uninterested in. This also means I skipped _ symbol though, which won't be in a common ngram for my use case.\n",
    "\n",
    "### Merging method\n",
    "\n",
    "I then created another priority queue with a size of 10,000 and went through each file. I used 2 priority queues to take ngrams with and without symbols or special characters. The files were saved.\n",
    "\n",
    "A 2gram without symbols or special characters file:\n",
    "    (5300515095, 'of the')\n",
    "    (2950334073, 'in the')\n",
    "    (1939208961, 'to the')\n",
    "    (1250796674, 'and the')\n",
    "    (1191540665, 'on the')\n",
    "    (949312195, 'for the')\n",
    "    (890209782, 'to be')\n",
    "    (786258968, 'of a')\n",
    " \n",
    "### The data\n",
    "\n",
    "The data files are too large, so I only included the 1 gram and 2 gram with no symbols data. If you want access to the raw data create an issue and i'll find another way to get it.\n",
    " \n",
    "### Future work\n",
    "\n",
    "Right now, I have only collected ngrams for n = 1 and 2. It takes a lot of time to download and unzip the files. In the future I would like to use a cloud provider and create several instances at once to speed the job up/ make it feasible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import gzip\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import heapq\n",
    "import sys\n",
    "from collections import deque\n",
    "from lxml import html\n",
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self, max_size):\n",
    "        self.queue = []\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def push(self, priority, item):\n",
    "        if len(self.queue) < self.max_size:\n",
    "            heapq.heappush(self.queue, (priority, item))\n",
    "        elif priority > self.queue[0][0]:\n",
    "            heapq.heapreplace(self.queue, (priority, item))\n",
    "\n",
    "    def pop(self):\n",
    "        return heapq.heappop(self.queue)\n",
    "    \n",
    "    def heapsort(self):\n",
    "        # reverses with q so I can merge max data more easily\n",
    "        q = deque()\n",
    "        while self.queue:\n",
    "            q.appendleft(self.pop())\n",
    "        return q\n",
    "    \n",
    "def process(pq, lines, n):\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').split()\n",
    "        ngram = ' '.join(line[0:n])\n",
    "        if '_' in ngram:\n",
    "        # Side effect, removes all instances of _ as well a _POS tags\n",
    "            continue\n",
    "        total_occurrence = 0\n",
    "        for data_by_date in line[n:]:\n",
    "            date, occurrence, _ = data_by_date.split(',')\n",
    "            if int(date) >= 1990: # newer content\n",
    "                total_occurrence += int(occurrence)  \n",
    "        pq.push(total_occurrence, ngram)\n",
    "\n",
    "def get_ngram_urls(n):\n",
    "    # datasets were generated in February 2020 on 17th\n",
    "    url = f'http://storage.googleapis.com/books/ngrams/books/20200217/eng/eng-{n}-ngrams_exports.html'\n",
    "    response = requests.get(url)\n",
    "    webpage = html.fromstring(response.content)\n",
    "    links = webpage.xpath('//a/@href')\n",
    "    return links[2:-5] # header/footer links\n",
    "\n",
    "def get_file(url):\n",
    "    response = requests.get(url, stream=True)\n",
    "    buffer_data =io.BytesIO(response.content)\n",
    "    gzip_fd = gzip.GzipFile(fileobj=buffer_data)\n",
    "    return gzip_fd\n",
    "\n",
    "def write_file(file_name, data):\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(data, fp)\n",
    "        \n",
    "def get_files(n):\n",
    "    return [file for file in glob.glob(f'{n}gram*.txt')]\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "    return data\n",
    "\n",
    "def has_symbols(s):\n",
    "    return not s.replace(' ', '').isalpha()\n",
    "\n",
    "def get_most_frequent_ngram(n):\n",
    "    '''Data is merged together and split depending on if there are symbols in the ngram'''\n",
    "    pq = PriorityQueue(10000)\n",
    "    pq_symbols = PriorityQueue(10000)\n",
    "    files = get_files(n)\n",
    "    for file_name in files:\n",
    "        data = read_file(file_name)\n",
    "        for line in data:\n",
    "            total_occurrence, ngram = line\n",
    "            if has_symbols(ngram):\n",
    "                pq_symbols.push(total_occurrence, ngram)\n",
    "            else:\n",
    "                pq.push(total_occurrence, ngram)\n",
    "            \n",
    "    data = pq.heapsort()\n",
    "    data_symbols = pq_symbols.heapsort()\n",
    "    return data, data_symbols\n",
    "\n",
    "def extract_ngrams_to_files(n):\n",
    "    '''This function extracts the ngram and creates a file of the top 1000 ngram for the file'''\n",
    "    BUF_SIZE = 20000\n",
    "    urls = get_ngram_urls(n)\n",
    "    for index, url in enumerate(urls):\n",
    "        pq = PriorityQueue(1000)\n",
    "        bigfile = get_file(url)\n",
    "        tmp_lines = bigfile.readlines(BUF_SIZE)\n",
    "        while tmp_lines:\n",
    "            process(pq, tmp_lines, n)\n",
    "            tmp_lines = bigfile.readlines(BUF_SIZE)\n",
    "\n",
    "        data = pq.heapsort()\n",
    "        file_name = f'{str(n)}gram-{str(index)}.txt'\n",
    "        write_file(file_name, data)\n",
    "        \n",
    "def merge_ngrams_from_files(n):\n",
    "    '''This function extracts the ngram and creates the top 10000 n gram with and without symbols'''\n",
    "    data, data_symbols = get_most_frequent_ngram(n)\n",
    "    file_name_data = f'{n}grams_no_symbols.txt'\n",
    "    file_name_data_symbols = f'{n}grams_all_symbols.txt'\n",
    "    write_file(file_name_data, data)\n",
    "    write_file(file_name_data_symbols, data_symbols)\n",
    "\n",
    "def main(n):\n",
    "    extract_ngrams_to_files(n)\n",
    "    merge_ngrams_from_files(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to to get just the ngrams in sorted order\n",
    "import pickle\n",
    "def read_file(file_name):\n",
    "    with open (file_name, 'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "    return data\n",
    "\n",
    "data = read_file('1grams_no_symbols.txt')\n",
    "ngrams_data = [ngram for _, ngram in data]\n",
    "\n",
    "with open('1gram.txt', 'w') as f:\n",
    "    for line in ngrams_data:\n",
    "        weird_symbol = max([ord(char) for char in line])\n",
    "        if weird_symbol < 122: # 122 is z\n",
    "            f.write(line+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
